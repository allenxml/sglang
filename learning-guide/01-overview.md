# 📋 SGLang 全景概览

> **目标**：读完本文，你将了解 SGLang 是什么、能做什么、项目是怎么组织的。

---

## 1. SGLang 是什么？

**一句话定义**：SGLang 是一个高性能的大语言模型（LLM）推理和服务引擎。

**用日常语言说**：

当你在网页上和 ChatGPT 聊天时，你打字发送问题，几秒后看到回答。这背后发生了什么？

1. 你的文字被发送到服务器
2. 服务器上的"推理引擎"把文字交给大模型
3. 大模型计算出回答
4. 回答被发送回你的屏幕

SGLang 就是第 2 步中的"推理引擎"。它不是大模型本身（不是厨师），而是让大模型高效工作的"餐厅管理系统"。

### 使用场景

- **API 服务**：把大模型部署为 HTTP 服务，多人同时使用
- **批量处理**：一次性处理大量文本（翻译、摘要等）
- **本地推理**：在自己的 GPU 服务器上运行开源模型

---

## 2. SGLang vs 其他推理引擎

| 特性 | SGLang | vLLM | TensorRT-LLM |
|------|--------|------|---------------|
| **开发语言** | Python | Python | C++/Python |
| **上手难度** | ⭐⭐ 简单 | ⭐⭐ 简单 | ⭐⭐⭐⭐ 较难 |
| **核心优化** | RadixAttention | PagedAttention | 编译优化 |
| **特色能力** | 前缀缓存复用、结构化输出 | 灵活调度 | 极致性能 |
| **适用场景** | 多轮对话、共享前缀 | 通用推理 | 生产环境极致优化 |

**SGLang 的独特之处**：
- **RadixAttention**：用一种聪明的缓存方式（类似浏览器缓存），让重复的"问题前缀"不需要重新计算
- **高性能调度**：像高效的餐厅调度系统，最大化 GPU 利用率

---

## 3. 项目目录结构

```
sglang/
├── python/sglang/
│   ├── srt/                    # 🔥 核心推理运行时（SRT = SGLang Runtime）
│   │   ├── entrypoints/        #   入口点：HTTP 服务器、引擎启动
│   │   │   ├── http_server.py  #     HTTP 服务和 API 路由
│   │   │   └── engine.py       #     引擎初始化、子进程启动
│   │   ├── managers/           #   管理器：各种核心组件
│   │   │   ├── tokenizer_manager.py   # 分词管理器
│   │   │   ├── scheduler.py           # 调度器
│   │   │   ├── schedule_batch.py      # 批次数据结构
│   │   │   ├── tp_worker.py           # 张量并行工作进程
│   │   │   ├── detokenizer_manager.py # 反分词管理器
│   │   │   └── io_struct.py           # 进程间通信数据结构
│   │   ├── model_executor/     #   模型执行器
│   │   │   └── model_runner.py #     GPU 上运行模型的核心
│   │   ├── models/             #   各种模型的适配代码
│   │   ├── layers/             #   模型层的实现（Attention 等）
│   │   ├── mem_cache/          #   内存和 KV 缓存管理
│   │   └── server_args.py      #   服务器启动参数定义
│   └── lang/                   # SGLang 前端语言（不在本指南范围内）
├── test/                       # 测试文件
├── docs/                       # 官方文档
└── learning-guide/             # 📚 你正在阅读的学习指南
```

**重点关注 `srt/` 目录**——这是 SGLang Runtime（运行时）的缩写，所有推理相关的核心代码都在这里。

---

## 4. 核心概念预览

### 4.1 什么是"推理"（Inference）？

**训练** vs **推理**——两个完全不同的阶段：

| | 训练（Training） | 推理（Inference） |
|--|-----------------|-------------------|
| 比喻 | 学生上课学习 | 学生参加考试答题 |
| 目的 | 让模型学会知识 | 让模型使用知识回答问题 |
| 计算量 | 极大（几天到几个月） | 较小（毫秒到秒级） |
| SGLang | ❌ 不负责 | ✅ 专门做这个 |

### 4.2 什么是 Token？

模型不认识"文字"，只认识"数字"。Token 就是文字和数字之间的桥梁。

```
"你好，世界" → 分词 → [123, 456, 789, 234]  （Token ID 序列）
                       ↓
                   模型计算
                       ↓
[567, 890]  → 反分词 → "你好"
```

> 💡 一个中文字大约是 1-2 个 Token，一个英文单词大约是 1-3 个 Token。

### 4.3 什么是 GPU？

GPU（图形处理器）最初是为游戏设计的，但它擅长"大量简单计算同时进行"，恰好符合大模型的需求。

- **CPU**：像一个超级聪明的人，一次做一件复杂的事
- **GPU**：像一千个普通工人，同时做一千件简单的事

大模型推理本质上就是大量的矩阵乘法——这正是 GPU 的强项。

### 4.4 推理的两个阶段

1. **Prefill（预填充）**：模型一次性"读完"你的所有输入文字
   - 比喻：考试时先通读整道题
2. **Decode（解码）**：模型一个 Token 一个 Token 地生成回答
   - 比喻：读完题后一个字一个字写答案

---

## 5. 下一步

现在你已经对 SGLang 有了全面的初步印象。接下来：

- 想了解系统架构 → [架构设计](02-architecture.md)
- 想直接看请求流程 → [推理请求的旅程](04-request-journey.md)
- 遇到不懂的词 → [术语表](10-glossary.md)
