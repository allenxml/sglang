# 🧠 模型推理

> **目标**：理解 ModelRunner 如何在 GPU 上执行模型的前向传播，以及 CUDA Graph 优化是什么。

---

## 1. 什么是"前向传播"（Forward Pass）？

### 比喻：流水线加工 🏭

想象一个多层的加工流水线：
- **原材料**（输入 Token）从入口进入
- 经过第 1 层加工（第 1 层 Transformer）
- 经过第 2 层加工（第 2 层 Transformer）
- ...
- 经过第 N 层加工（第 N 层 Transformer）
- **成品**（输出概率分布）从出口出来

```
输入 Token IDs: [1234, 5678, 9012]
        │
        ▼
┌─────────────────┐
│  Embedding 层    │  ← Token ID → 向量（数字列表）
│  (查字典)        │
└────────┬────────┘
         │  向量 [0.12, -0.34, 0.56, ...]
         ▼
┌─────────────────┐
│ Transformer 第1层│  ← 自注意力 + 前馈网络
└────────┬────────┘
         ▼
┌─────────────────┐
│ Transformer 第2层│
└────────┬────────┘
         ▼
       ... (重复 N 层)
         │
         ▼
┌─────────────────┐
│ Transformer 第N层│
└────────┬────────┘
         ▼
┌─────────────────┐
│    LM Head       │  ← 向量 → 词表概率分布
│  (输出层)        │
└────────┬────────┘
         ▼
输出: logits [0.01, 0.003, ..., 0.15, ...]
      (每个词的概率，词表大小通常 32000~128000)
```

> 💡 "前向传播"就是数据从输入到输出"正向"通过模型的过程。（训练时还有"反向传播"，但推理不需要。）

---

## 2. ModelRunner 的职责

**文件**：`python/sglang/srt/model_executor/model_runner.py`

ModelRunner 就像 GPU 上的"总指挥"：

```
ModelRunner 的工作：
│
├── 初始化阶段（启动时执行一次）
│   ├── 加载模型权重到 GPU
│   ├── 分配 KV Cache 显存
│   └── 预热 CUDA Graph
│
└── 推理阶段（每个批次执行一次）
    ├── forward_extend()  ← Prefill：处理多个输入 Token
    └── forward_decode()  ← Decode：只处理 1 个新 Token
```

### forward_extend()（Prefill 阶段）

当一个新请求进来，需要一次性处理所有输入 Token：

```
输入: "什么是人工智能？" → [T1, T2, T3, T4, T5, T6]
                              │
                    forward_extend()
                              │
                              ▼
输出: KV Cache 已填充 + 第一个输出 Token 的概率
```

**特点**：
- 输入 Token 数量多，计算量大
- 可以利用 GPU 的并行计算能力
- 主要瓶颈是**计算**

### forward_decode()（Decode 阶段）

每次只处理一个新 Token：

```
输入: 上一步生成的 Token [T7] + KV Cache
                              │
                    forward_decode()
                              │
                              ▼
输出: 下一个 Token 的概率
```

**特点**：
- 只有 1 个新 Token，计算量小
- 但需要读取整个 KV Cache
- 主要瓶颈是**内存带宽**

---

## 3. GPU 计算基础

### 为什么用 GPU？

大模型推理的核心操作是**矩阵乘法**——将两个大矩阵相乘。

```
简化示例：
输入向量: [0.1, 0.2, 0.3, 0.4]     （1×4 的矩阵）
权重矩阵: [[0.1, 0.2],              （4×2 的矩阵）
           [0.3, 0.4],
           [0.5, 0.6],
           [0.7, 0.8]]
                 ↓ 矩阵乘法
输出向量: [0.5, 0.6]                 （1×2 的矩阵）
```

真实模型中，这些矩阵可能有几千到几万维，需要做海量的乘法和加法。

- **CPU**：串行计算，一次做一个乘法
- **GPU**：并行计算，同时做几千个乘法

> 💡 **比喻**：计算 1000 道乘法题——CPU 是一个数学天才一题一题做；GPU 是 1000 个普通学生同时做，每人一题。虽然 GPU 每个"工人"都没 CPU 快，但人多力量大。

### 张量（Tensor）

在深度学习中，所有数据都以"张量"形式存在：
- **标量**（0 维）：一个数，如 `3.14`
- **向量**（1 维）：一列数，如 `[1.0, 2.0, 3.0]`
- **矩阵**（2 维）：一张表，如 `[[1, 2], [3, 4]]`
- **张量**（N 维）：更高维度的数组

模型的权重、输入、输出都是张量。

---

## 4. CUDA Graph 优化

### 问题：GPU 启动开销

每次调用 GPU 计算时，CPU 需要做一些准备工作：
1. 准备输入数据
2. 告诉 GPU "请执行这个计算"
3. 等待 GPU 完成
4. 取回结果

这个"启动过程"（launch overhead）虽然很短（微秒级），但 Decode 阶段每个 Token 都要来一次，积少成多就很可观。

### 解决方案：CUDA Graph

CUDA Graph 就像"录像+回放"：

```
第一次执行：
  CPU 告诉 GPU: "做计算A → 做计算B → 做计算C"
  GPU 执行并记录整个过程 📹

后续执行：
  CPU 告诉 GPU: "回放录像"
  GPU 直接按照录像执行 ▶️
  （省去了每次重新下达指令的时间）
```

**效果**：
- 减少 CPU-GPU 之间的通信次数
- Decode 阶段可以提速 10-30%
- 特别适合 Decode（因为计算模式固定、重复执行）

> 💡 **比喻**：就像你每天做同样的早餐——第一次需要看菜谱一步步做，之后就不看菜谱了，闭着眼都能做，速度快很多。

### CUDA Graph 的限制

- 需要固定的输入形状（批次大小、序列长度）
- SGLang 为常用的批次大小预先录制多个 Graph
- 如果当前批次大小没有预录的 Graph，就用普通方式执行

---

## 5. 张量并行（Tensor Parallelism）

当模型太大、一块 GPU 放不下时，需要把模型"切开"放到多块 GPU 上：

```
单 GPU（模型放得下）:
┌────────────────────┐
│      GPU 0          │
│  [完整模型权重]      │
└────────────────────┘

张量并行 tp=2（模型太大，需要 2 块 GPU）:
┌────────────────────┐  ┌────────────────────┐
│      GPU 0          │  │      GPU 1          │
│  [模型权重左半]      │  │  [模型权重右半]      │
└─────────┬──────────┘  └──────────┬─────────┘
          │                        │
          └──── 计算后同步 ────────┘
```

**文件**：`python/sglang/srt/managers/tp_worker.py`

TpWorker（张量并行工作器）负责：
1. 每个 GPU 上加载模型的一部分
2. 每层计算后，GPU 之间同步中间结果
3. 保证最终输出和单 GPU 完全一致

> 💡 **比喻**：一个蛋糕太大，一个烤箱放不下，就切成两半，用两个烤箱分别烤，最后再拼起来。

---

## 6. 模型权重加载

启动时，ModelRunner 需要把模型权重从磁盘加载到 GPU：

```
步骤 1: 找到模型文件
        /path/to/model/
        ├── config.json          ← 模型配置（层数、维度等）
        ├── model-00001.safetensors  ← 权重文件 1
        ├── model-00002.safetensors  ← 权重文件 2
        └── tokenizer.json       ← 分词器

步骤 2: 读取 config.json，确定模型架构

步骤 3: 根据架构创建模型实例（空壳）

步骤 4: 把权重参数填入模型
        （safetensors → GPU 张量）

步骤 5: 模型就绪 ✅
```

---

## 7. 下一步

- 了解 KV Cache 的原理 → [KV 缓存](08-kv-cache.md)
- 了解采样过程 → [采样与生成](09-sampling.md)
- 返回目录 → [README](README.md)
