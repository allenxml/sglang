# 💾 KV 缓存（KV Cache）

> **目标**：理解 KV Cache 是什么、为什么重要、RadixAttention 如何优化前缀共享。

---

## 1. 什么是 KV Cache？

### 比喻：考试草稿纸 📝

想象你在做一道数学大题：
- 第 1 小问：你计算了很多中间步骤，写在草稿纸上
- 第 2 小问：需要用到第 1 小问的中间结果
- **如果没有草稿纸**：每做一小问都要从头算，重复大量工作
- **有了草稿纸**：直接查看之前的结果，继续往下算

KV Cache 就是大模型的"草稿纸"——存储每个 Token 在每一层 Transformer 的中间计算结果（Key 和 Value 向量），供后续 Token 使用。

### 技术解释

在 Transformer 的自注意力（Self-Attention）机制中：
- 每个 Token 会产生三个向量：**Q**（Query，查询）、**K**（Key，键）、**V**（Value，值）
- 新 Token 需要和所有之前 Token 的 K、V 做计算
- 如果不缓存，每生成一个新 Token 都要重新算之前所有 Token 的 K、V

```
不用 KV Cache（每次都重算）:
Token 1: 计算 K1, V1
Token 2: 重新计算 K1, V1, K2, V2      ← 重复计算了 K1, V1
Token 3: 重新计算 K1, V1, K2, V2, K3, V3  ← 又重复了
...
计算量: O(N²)  😱

使用 KV Cache（只算新的）:
Token 1: 计算 K1, V1 → 存入缓存
Token 2: 只算 K2, V2 → 存入缓存，复用缓存中的 K1, V1
Token 3: 只算 K3, V3 → 存入缓存，复用缓存中的 K1, V1, K2, V2
...
计算量: O(N)   😊
```

> 💡 **简单说**：KV Cache 让 Decode 阶段每步只需计算 1 个 Token 的 K、V，而不是重新计算所有 Token 的。这就是为什么 Decode 可以很快。

---

## 2. KV Cache 的大小

KV Cache 的显存占用是可观的：

```
KV Cache 大小 = 2 × 层数 × 注意力头数 × 头维度 × 序列长度 × 数据类型大小

以 Llama-3-8B 为例：
= 2 × 32层 × 32头 × 128维 × 4096序列 × 2字节(FP16)
≈ 2 GB（一个请求！）
```

如果同时处理 100 个请求，KV Cache 就需要 ~200 GB！这就是为什么**内存管理**至关重要。

### 显存分配图

```
GPU 显存 (如 80 GB):
┌──────────────────────────────────────────┐
│  模型权重         │     KV Cache         │ 临时 │
│  (~16 GB)        │     (~50 GB)         │ (~14GB)│
│  [固定不变]       │     [动态分配]        │       │
└──────────────────────────────────────────┘
```

---

## 3. KV Cache 的内存管理

### 问题：碎片化

不同请求的序列长度不同，频繁分配和释放会产生"内存碎片"：

```
碎片化的显存：
[请求A的Cache] [空闲] [请求B的Cache] [空闲] [请求C的Cache] [空闲]
                 ↑ 虽然总共有足够空间，但不连续，无法使用
```

### 解决方案：分页管理

SGLang 使用类似操作系统"虚拟内存"的方式管理 KV Cache：
- 将显存划分为固定大小的"页"（Page/Block）
- 每个请求按需分配页，不需要连续
- 请求完成后释放页供其他请求使用

```
分页管理的显存：
页号: [1] [2] [3] [4] [5] [6] [7] [8]
分配: [A] [A] [B] [空] [B] [C] [空] [C]

请求 A 使用页: 1, 2
请求 B 使用页: 3, 5    ← 不需要连续！
请求 C 使用页: 6, 8
```

---

## 4. RadixAttention — SGLang 的核心创新

### 什么是前缀共享？

很多请求会有相同的开头（前缀）：

```
请求 1: "你是一个翻译助手。请翻译：Hello"
请求 2: "你是一个翻译助手。请翻译：World"
请求 3: "你是一个翻译助手。请翻译：Good morning"
         ↑─────── 共同前缀 ─────────↑  ↑── 不同部分 ──↑
```

这三个请求的前缀完全相同！如果每个请求都独立计算前缀的 KV Cache，就浪费了大量计算和显存。

### RadixAttention 的解决方案

SGLang 使用 **Radix Tree**（基数树）来管理 KV Cache，让共享前缀的请求复用同一份缓存：

```
Radix Tree 结构：

                    [系统提示: "你是一个翻译助手"]
                              │
                    [共同前缀: "请翻译："]
                     ╱          │           ╲
              ["Hello"]    ["World"]    ["Good morning"]
              (请求1)      (请求2)       (请求3)
```

**效果**：
- 共同前缀的 KV Cache 只计算和存储一次
- 新请求如果匹配已有前缀，直接复用，跳过 Prefill
- 节省大量计算时间和显存

> 💡 **比喻**：就像浏览器缓存——你第一次打开网站，图片要下载；第二次打开，图片直接从缓存读取，快多了。RadixAttention 就是 LLM 推理的"浏览器缓存"。

### 哪些场景受益？

| 场景 | 共同前缀 | 收益 |
|------|---------|------|
| 多轮对话 | 之前的对话历史 | ⭐⭐⭐ 极大 |
| 批量翻译 | 相同的系统提示 | ⭐⭐ 较大 |
| Few-shot 学习 | 相同的示例 | ⭐⭐⭐ 极大 |
| 独立请求 | 几乎无共同前缀 | ⭐ 较小 |

---

## 5. 缓存淘汰策略

GPU 显存有限，不可能缓存所有请求的 KV Cache。当显存满时需要淘汰：

```
缓存淘汰的决策：
│
├── LRU（最近最少使用）
│   └── 最久没被访问的缓存优先淘汰
│
├── 前缀保留
│   └── 被多个请求共享的前缀优先保留
│
└── 大小考虑
    └── 超长序列的缓存可能被优先淘汰（释放空间多）
```

> 💡 **比喻**：就像冰箱空间有限——最近常吃的食材放前面，很久没用的调料清理掉，给新食材腾地方。

---

## 6. KV Cache 的未来

KV Cache 是当前 LLM 推理的最大瓶颈之一，研究方向包括：
- **量化**：用更少的比特存储 K、V（如 FP16 → INT8）
- **压缩**：丢弃不重要的 K、V
- **稀疏化**：只保留关键位置的 K、V

---

## 7. 下一步

- 了解采样过程 → [采样与生成](09-sampling.md)
- 返回请求流程全貌 → [推理请求的旅程](04-request-journey.md)
- 返回目录 → [README](README.md)
