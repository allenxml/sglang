# 📖 术语表

> 本术语表按字母顺序排列，方便查阅。每个术语都有中文解释和日常比喻。

---

## A

### Attention（注意力机制）
Transformer 模型的核心机制。让模型在处理一个词时，能"关注"到输入中其他相关的词。
- **比喻**：阅读理解时，读到"他"这个字，你会回头看看"他"指的是谁——这就是注意力机制。

### Auto-regressive（自回归）
模型一次只生成一个 Token，每个新 Token 都依赖于之前生成的所有 Token。
- **比喻**：写作文时一个字一个字写，每个字都要看看前面写了什么。

---

## B

### Batch / Batching（批次 / 批处理）
把多个请求打包在一起，送到 GPU 同时处理。
- **比喻**：洗衣服时把多件衣服一起放进洗衣机，比一件一件洗高效多了。
- **相关**：→ Continuous Batching

### BPE（Byte Pair Encoding，字节对编码）
一种常见的分词算法，通过反复合并最常见的字符对来构建词表。
- **示例**："unhappiness" → ["un", "happiness"] → Token IDs

---

## C

### Chat Template（聊天模板）
把用户消息（messages）转换为模型期望的特定文本格式。每个模型有自己的模板。
- **比喻**：不同国家的信件格式不同，聊天模板确保"信件格式"正确。

### Continuous Batching（连续批处理）
一种高效的批处理策略：不用等一批请求全部完成再处理下一批，新请求可以随时加入，完成的请求随时离开。
- **比喻**：公交车模式——到站上人、到站下人，不用等所有乘客都到了才发车。
- **对比**：Static Batching 是旅行团模式——等所有人到齐才出发。

### CUDA
NVIDIA GPU 的编程平台。SGLang 通过 CUDA 在 GPU 上执行模型计算。
- **注意**：这意味着 SGLang 主要支持 NVIDIA GPU。

### CUDA Graph
一种 GPU 优化技术，把一系列 GPU 操作"录制"下来，后续直接"回放"，减少 CPU-GPU 之间的通信开销。
- **比喻**：第一次做菜看菜谱，录下步骤后，以后不看菜谱直接做，更快。

---

## D

### Decode（解码阶段）
推理的第二阶段。模型一个 Token 一个 Token 地生成回答。
- **比喻**：读完考题后，一个字一个字写答案。
- **特点**：每步只产生 1 个 Token，但需要重复很多次。
- **对比**：→ Prefill

### Detokenization（反分词）
把 Token ID 序列转换回人类可读的文字。分词的逆过程。
- **示例**：[12345, 67890] → "你好"

---

## E

### Embedding（嵌入）
把 Token ID 转换为高维向量（一串浮点数）。这是模型处理的第一步。
- **比喻**：把"门牌号"转换为"GPS 坐标"——数字编号变成模型能理解的数学表示。
- **示例**：Token ID 12345 → [0.12, -0.34, 0.56, ..., 0.78]（几千维的向量）

### EOS（End of Sequence，序列结束标记）
一个特殊 Token，表示"我说完了"。模型生成 EOS 时，推理结束。
- **比喻**：写信最后的"此致敬礼"。

---

## F

### FastAPI
一个 Python Web 框架，SGLang 用它来创建 HTTP 服务。
- **作用**：让 SGLang 可以通过 HTTP 接口接收和响应请求。

### Forward Pass（前向传播）
数据从模型的输入层到输出层的一次完整计算过程。
- **比喻**：工厂流水线，原材料从入口进去，经过层层加工，成品从出口出来。

---

## G

### GPU（Graphics Processing Unit，图形处理器）
擅长大规模并行计算的芯片，是大模型推理的主力硬件。
- **比喻**：CPU 是一个超级聪明的人；GPU 是一千个普通工人同时干活。

### Greedy Decoding（贪心解码）
每次选择概率最高的词作为下一个 Token。结果确定但缺乏多样性。
- **对比**：→ Sampling

---

## I

### Inference（推理）
让训练好的模型根据输入生成输出的过程。SGLang 专门负责这个阶段。
- **比喻**：训练是"上课学习"，推理是"考试答题"。
- **对比**：Training（训练）

---

## K

### KV Cache（KV 缓存）
存储 Transformer 模型每层的 Key 和 Value 向量，避免 Decode 阶段重复计算。
- **比喻**：做数学题的草稿纸——记下中间步骤，后面直接用，不用从头算。
- **详细**：→ [KV 缓存详解](08-kv-cache.md)

---

## L

### Latency（延迟）
从发出请求到收到回答的时间。
- **TTFT**（Time To First Token）：收到第一个词的时间
- **TPOT**（Time Per Output Token）：每个词的生成时间

### LLM（Large Language Model，大语言模型）
参数量巨大的语言模型，如 GPT-4、LLaMA、Qwen 等。SGLang 是让 LLM 高效工作的引擎。

### Logits
模型前向传播的原始输出，是每个词的"得分"（未经 softmax 归一化）。经过 softmax 后变为概率分布。

---

## M

### Max Tokens
限制模型最多生成的 Token 数量。
- **示例**：max_tokens=100 表示最多生成 100 个 Token。

### Model Runner（模型运行器）
在 GPU 上实际执行模型前向传播的组件。
- **文件**：`python/sglang/srt/model_executor/model_runner.py`

---

## P

### Prefill（预填充阶段）
推理的第一阶段。模型一次性处理所有输入 Token。
- **比喻**：考试时先通读整道题目。
- **特点**：只执行一次，但计算量大（输入越长越慢）。
- **对比**：→ Decode

### Prompt（提示）
用户发送给模型的输入文本。
- **示例**："请介绍一下人工智能" 就是一个 prompt。

---

## R

### RadixAttention
SGLang 的核心创新。使用 Radix Tree（基数树）管理 KV Cache，让共享前缀的请求复用同一份缓存。
- **效果**：多轮对话、相同系统提示等场景大幅加速。
- **详细**：→ [KV 缓存详解](08-kv-cache.md)

### Req / Request（请求）
一个用户的推理请求。在系统内部表示为 `Req` 对象。

---

## S

### Sampling（采样）
从模型输出的概率分布中选择下一个 Token 的过程。
- **详细**：→ [采样与生成](09-sampling.md)

### Scheduler（调度器）
管理请求队列，决定哪些请求组成批次送去 GPU 计算。
- **比喻**：餐厅经理，决定先做谁的菜。
- **文件**：`python/sglang/srt/managers/scheduler.py`

### Streaming（流式输出）
边生成边返回，用户可以看到回答像打字一样逐步出现。
- **比喻**：ChatGPT 的"打字效果"。

---

## T

### Temperature（温度）
控制采样随机性的参数。温度越高越随机，越低越确定。
- **温度 = 0**：总是选最可能的词（贪心）
- **温度 = 0.7**：常用值，自然流畅
- **温度 > 1**：非常随机，可能不连贯

### Tensor（张量）
多维数组，深度学习中所有数据的基本表示形式。
- 标量（0维）、向量（1维）、矩阵（2维）、更高维张量

### Tensor Parallelism / TP（张量并行）
把一个模型切分到多块 GPU 上，每块 GPU 负责一部分计算。
- **比喻**：蛋糕太大一个烤箱放不下，切开用两个烤箱分别烤。
- **参数**：`--tp-size 2` 表示使用 2 块 GPU。

### Token
文字的最小处理单位。模型不直接处理文字，而是处理 Token。
- **示例**：英文每 4 字符 ≈ 1 Token；中文每 1-2 字 ≈ 1 Token。
- **详细**：→ [分词与反分词](05-tokenization.md)

### Tokenization（分词）
把文字转换为 Token ID 序列的过程。
- **示例**："你好" → [12345, 67890]

### Top-K
采样时只考虑概率最高的 K 个词。
- **示例**：Top-K=3 表示只从前 3 名候选词中选。

### Top-P（Nucleus Sampling）
采样时只考虑累计概率达到 P 的最少候选词。
- **示例**：Top-P=0.9 表示选到累计概率超过 90% 的候选词为止。

### Transformer
当前大语言模型的主流架构。由 Google 于 2017 年提出（"Attention is All You Need"）。
- **核心组件**：自注意力（Self-Attention）+ 前馈网络（FFN）

### Throughput（吞吐量）
单位时间内能处理的 Token 数量，衡量系统的处理能力。
- **单位**：tokens/second

---

## Z

### ZMQ（ZeroMQ）
轻量级消息传递库，SGLang 用它在进程之间传递消息。
- **比喻**：进程之间的"对讲机"。
- **为什么用它**：简单、高效、支持多种通信模式。

---

## 📌 补充参考

- SGLang 官方文档：`docs/` 目录
- 返回目录 → [README](README.md)
