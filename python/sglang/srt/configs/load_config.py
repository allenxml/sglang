# ================================================================================
# ðŸ“¦ æ¨¡åž‹åŠ è½½é…ç½® (Load Config)
# ================================================================================
#
# ã€è¿™ä¸ªæ–‡ä»¶æ˜¯ä»€ä¹ˆã€‘What This File Does
# è¿™ä¸ªæ–‡ä»¶å®šä¹‰äº†æ¨¡åž‹æƒé‡åŠ è½½çš„é…ç½®ç±»ï¼ˆLoadConfigï¼‰ï¼ŒæŽ§åˆ¶å¦‚ä½•ä»Žç£ç›˜/HuggingFace Hub
# åŠ è½½æ¨¡åž‹æƒé‡ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼ˆsafetensorsã€pytorchã€GGUFã€é‡åŒ–æ ¼å¼ç­‰ï¼‰ã€‚
#
# ã€ç”Ÿæ´»æ¯”å–»ã€‘Metaphor
# æƒ³è±¡è¿™æ˜¯ä¸€ä¸ª"å›¾ä¹¦é¦†ä¹¦ç±æå–è§„åˆ™"ï¼š
# - LoadConfig = å›¾ä¹¦ç®¡ç†å‘˜çš„å·¥ä½œæ‰‹å†Œ
# - load_format = ä¹¦ç±æ ¼å¼ï¼ˆç²¾è£…æœ¬ã€ç”µå­ç‰ˆã€æœ‰å£°ä¹¦ç­‰ï¼‰
# - download_dir = ä¹¦åº“ä½ç½®
# - decryption_key = åŠ å¯†ä¹¦ç±çš„å¯†é’¥
#
# ã€æ ¸å¿ƒé…ç½®ã€‘Key Configurations
# 1. load_format: æƒé‡æ–‡ä»¶æ ¼å¼
#    - auto: è‡ªåŠ¨æ£€æµ‹ï¼ˆä¼˜å…ˆ safetensorsï¼Œå›žé€€åˆ° ptï¼‰
#    - safetensors: HuggingFace æŽ¨èæ ¼å¼ï¼ˆå®‰å…¨ã€å¿«é€Ÿï¼‰
#    - pt: PyTorch åŽŸç”Ÿæ ¼å¼ï¼ˆ.bin æ–‡ä»¶ï¼‰
#    - gguf: llama.cpp æ ¼å¼ï¼ˆé‡åŒ–æ¨¡åž‹ï¼‰
#    - bitsandbytes: NF4/INT8 é‡åŒ–æ ¼å¼
#
# 2. download_dir: æ¨¡åž‹æƒé‡ä¸‹è½½/ç¼“å­˜ç›®å½•
#    - é»˜è®¤ï¼š~/.cache/huggingface/hub
#    - å¯è‡ªå®šä¹‰ï¼ˆå¦‚æŒ‚è½½çš„ NFS å…±äº«ç›®å½•ï¼‰
#
# 3. model_loader_extra_config: é¢å¤–åŠ è½½å‚æ•°ï¼ˆJSON æ ¼å¼ï¼‰
#    - ç”¨äºŽç‰¹æ®Šæ¨¡åž‹çš„è‡ªå®šä¹‰åŠ è½½é€»è¾‘
#
# 4. é‡åŒ–é…ç½®:
#    - modelopt_config: ModelOpt é‡åŒ–é…ç½®
#    - rl_quant_profile: RL é‡åŒ– profile æ–‡ä»¶è·¯å¾„
#
# ã€ä½¿ç”¨ç¤ºä¾‹ã€‘Usage
# åŠ è½½ AWQ é‡åŒ–æ¨¡åž‹ï¼š
#   python -m sglang.launch_server \
#     --model meta-llama/Llama-3.1-70B-Instruct-AWQ \
#     --load-format auto \
#     --download-dir /mnt/models
#
# ================================================================================

# Adapted from https://github.com/vllm-project/vllm/blob/v0.6.4.post1/vllm/config.py
import enum
import logging
from dataclasses import dataclass, field
from typing import Any, List, Optional, Union

import orjson

from sglang.srt.configs.modelopt_config import ModelOptConfig
from sglang.srt.utils import is_hip

logger = logging.getLogger(__name__)


# ======== æ¨¡åž‹æƒé‡æ ¼å¼æžšä¸¾ ========
class LoadFormat(str, enum.Enum):
    """
    æ¨¡åž‹æƒé‡åŠ è½½æ ¼å¼

    ã€å¸¸ç”¨æ ¼å¼ã€‘
    - AUTO: è‡ªåŠ¨æ£€æµ‹ï¼ˆæŽ¨èï¼‰
    - SAFETENSORS: HuggingFace æŽ¨èæ ¼å¼ï¼ˆå®‰å…¨ã€é«˜æ•ˆï¼‰
    - PT: PyTorch åŽŸç”Ÿæ ¼å¼ï¼ˆ.bin æ–‡ä»¶ï¼‰
    - GGUF: llama.cpp é‡åŒ–æ ¼å¼
    - BITSANDBYTES: NF4/INT8 é‡åŒ–

    ã€ç‰¹æ®Šæ ¼å¼ã€‘
    - DUMMY: éšæœºåˆå§‹åŒ–æƒé‡ï¼ˆç”¨äºŽæ€§èƒ½æµ‹è¯•ï¼‰
    - NPCACHE: PyTorch + NumPy ç¼“å­˜ï¼ˆåŠ é€Ÿé‡å¤åŠ è½½ï¼‰
    - REMOTE: è¿œç¨‹æƒé‡åŠ è½½ï¼ˆè·¨èŠ‚ç‚¹ï¼‰
    """
    AUTO = "auto"  # è‡ªåŠ¨æ£€æµ‹
    PT = "pt"  # PyTorch æ ¼å¼ï¼ˆ.binï¼‰
    SAFETENSORS = "safetensors"  # SafeTensors æ ¼å¼ï¼ˆæŽ¨èï¼‰
    NPCACHE = "npcache"  # NumPy ç¼“å­˜
    DUMMY = "dummy"  # è™šæ‹Ÿæƒé‡ï¼ˆæ€§èƒ½æµ‹è¯•ç”¨ï¼‰
    SHARDED_STATE = "sharded_state"  # åˆ†ç‰‡çŠ¶æ€
    GGUF = "gguf"  # llama.cpp é‡åŒ–æ ¼å¼
    BITSANDBYTES = "bitsandbytes"  # BitsAndBytes é‡åŒ–
    MISTRAL = "mistral"  # Mistral æ ¼å¼
    LAYERED = "layered"  # åˆ†å±‚åŠ è½½
    FLASH_RL = "flash_rl"  # RL è®­ç»ƒé‡åŒ–æ¨¡åž‹ # For RL training with quantized models
    JAX = "jax"  # JAX æ ¼å¼
    REMOTE = "remote"  # è¿œç¨‹åŠ è½½
    REMOTE_INSTANCE = "remote_instance"  # è¿œç¨‹å®žä¾‹
    RDMA = "rdma"  # RDMA ä¼ è¾“
    LOCAL_CACHED = "local_cached"  # æœ¬åœ°ç¼“å­˜
    FASTSAFETENSORS = "fastsafetensors"  # å¿«é€Ÿ SafeTensors
    PRIVATE = "private"  # ç§æœ‰æ ¼å¼


@dataclass
class LoadConfig:
    """
    download_dir: Directory to download and load the weights, default to the
        default cache directory of huggingface.
    load_format: The format of the model weights to load:
        "auto" will try to load the weights in the safetensors format and
            fall back to the pytorch bin format if safetensors format is
            not available.
        "pt" will load the weights in the pytorch bin format.
        "safetensors" will load the weights in the safetensors format.
        "npcache" will load the weights in pytorch format and store
            a numpy cache to speed up the loading.
        "dummy" will initialize the weights with random values, which is
            mainly for profiling.
        "bitsandbytes" will load nf4 type weights.
        "flash_rl" will load weights with support for RL training
            with quantized models, enabling efficient weight reloading.
    ignore_patterns: The list of patterns to ignore when loading the model.
        Default to "original/**/*" to avoid repeated loading of llama's
        checkpoints.
    decryption_key_file: If set, decrypts the output files with a password read
        from this file (after PBKDF2).
    decrypt_max_concurrency: The maximum number of concurrent processes to decrypt the safetensor files. -1 means no limit.

    # ModelOpt-specific loading options
    modelopt_checkpoint_restore_path: Optional[str] = None
    modelopt_checkpoint_save_path: Optional[str] = None
    modelopt_export_path: Optional[str] = None
    """

    load_format: Union[str, LoadFormat] = LoadFormat.AUTO
    download_dir: Optional[str] = None
    model_loader_extra_config: Optional[Union[str, dict]] = field(default_factory=dict)
    ignore_patterns: Optional[Union[List[str], str]] = None
    decryption_key_file: Optional[str] = None
    decrypt_max_concurrency: int = -1
    tp_rank: Optional[int] = None
    remote_instance_weight_loader_seed_instance_ip: Optional[str] = None
    remote_instance_weight_loader_seed_instance_service_port: Optional[int] = None
    remote_instance_weight_loader_send_weights_group_ports: Optional[List[int]] = None
    remote_instance_weight_loader_backend: Optional[str] = None
    remote_instance_weight_loader_transfer_engine: Optional[Any] = None

    # ModelOpt-specific loading options
    modelopt_checkpoint_restore_path: Optional[str] = None
    modelopt_checkpoint_save_path: Optional[str] = None
    modelopt_export_path: Optional[str] = None

    # ModelOpt configuration object
    modelopt_config: Optional[ModelOptConfig] = None

    # QuantizedRL-specific options (for FlashRL-style quantization)
    rl_quant_profile: Optional[str] = (
        None  # Path to rollout quantization profile (e.g., /root/profile.7b.pt)
    )

    # For multi-layer MTP
    draft_model_idx: Optional[int] = None

    def __post_init__(self):
        model_loader_extra_config = self.model_loader_extra_config or {}
        if isinstance(model_loader_extra_config, str):
            self.model_loader_extra_config = orjson.loads(model_loader_extra_config)
        self._verify_load_format()

        if self.ignore_patterns is not None and len(self.ignore_patterns) > 0:
            logger.info(
                "Ignoring the following patterns when downloading weights: %s",
                self.ignore_patterns,
            )
        else:
            self.ignore_patterns = ["original/**/*"]

        # Create ModelOptConfig if not provided
        if self.modelopt_config is None:
            self.modelopt_config = ModelOptConfig(
                checkpoint_restore_path=self.modelopt_checkpoint_restore_path,
                checkpoint_save_path=self.modelopt_checkpoint_save_path,
                export_path=self.modelopt_export_path,
            )

    def _verify_load_format(self) -> None:
        if not isinstance(self.load_format, str):
            return

        load_format = self.load_format.lower()
        self.load_format = LoadFormat(load_format)

        rocm_not_supported_load_format: List[str] = []
        if is_hip() and load_format in rocm_not_supported_load_format:
            rocm_supported_load_format = [
                f
                for f in LoadFormat.__members__
                if (f not in rocm_not_supported_load_format)
            ]
            raise ValueError(
                f"load format '{load_format}' is not supported in ROCm. "
                f"Supported load formats are "
                f"{rocm_supported_load_format}"
            )
