# TPU

SGLang supports high-performance TPU inference through the SGLang-JAX backend, which is specifically optimized for Google Cloud TPUs. The JAX-based implementation delivers exceptional throughput and low latency for Large Language Model (LLM) serving workloads on TPU hardware.

For TPU-specific issues or feature requests, please visit the [sglang-jax GitHub issues page](https://github.com/sgl-project/sglang-jax/issues).

**NOTE:** SGLang TPU support is implemented via the SGLang-JAX backend, a dedicated JAX-based inference engine maintained as a separate repository at [https://github.com/sgl-project/sglang-jax](https://github.com/sgl-project/sglang-jax).

**ä¸­æ–‡å¯¹ç…§**ï¼š# TPU

SGLang é€šè¿‡ SGLang-JAX åç«¯æ”¯æŒé«˜æ€§èƒ½ TPU æ¨ç†ï¼Œè¯¥åç«¯é’ˆå¯¹ Google Cloud TPU è¿›è¡Œäº†ä¸“é—¨ä¼˜åŒ–ã€‚åŸºäº JAX çš„å®ç°ä¸º TPU ç¡¬ä»¶ä¸Šçš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æœåŠ¡å·¥ä½œè´Ÿè½½æä¾›äº†å“è¶Šçš„ååé‡å’Œä½å»¶è¿Ÿã€‚

å¯¹äº TPU ç‰¹å®šé—®é¢˜æˆ–åŠŸèƒ½è¯·æ±‚ï¼Œè¯·è®¿é—® [sglang-jax GitHub issues é¡µé¢](https://github.com/sgl-project/sglang-jax/issues)ã€‚

**æ³¨æ„ï¼š** SGLang TPU æ”¯æŒé€šè¿‡ SGLang-JAX åç«¯å®ç°ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“ç”¨çš„åŸºäº JAX çš„æ¨ç†å¼•æ“ï¼Œä½œä¸ºç‹¬ç«‹ä»“åº“åœ¨ [https://github.com/sgl-project/sglang-jax](https://github.com/sgl-project/sglang-jax) ç»´æŠ¤ã€‚

## System Requirements

### Supported TPU Hardware

| TPU Type | HBM Memory | Availability |
|----------|-----------|--------------|
| TPU v6e | 32 GB | Google Cloud |
| TPU v7 | 96 GB per core | Google Cloud |

### Software Requirements

- **Python:** 3.12 or higher
- **JAX:** Latest version with TPU support
- **Environment:** Google Cloud TPU VM or compatible TPU runtime
- **Optional:** SkyPilot for simplified cloud deployment

**ä¸­æ–‡å¯¹ç…§**ï¼š## ç³»ç»Ÿè¦æ±‚

### æ”¯æŒçš„ TPU ç¡¬ä»¶

| TPU ç±»å‹ | HBM å†…å­˜ | å¯ç”¨æ€§ |
|----------|-----------|--------------|
| TPU v6e | 32 GB | Google Cloud |
| TPU v7 | æ¯æ ¸å¿ƒ 96 GB | Google Cloud |

### è½¯ä»¶è¦æ±‚

- **Python:** 3.12 æˆ–æ›´é«˜ç‰ˆæœ¬
- **JAX:** æ”¯æŒ TPU çš„æœ€æ–°ç‰ˆæœ¬
- **ç¯å¢ƒ:** Google Cloud TPU VM æˆ–å…¼å®¹çš„ TPU è¿è¡Œæ—¶
- **å¯é€‰:** SkyPilot ç”¨äºç®€åŒ–çš„äº‘éƒ¨ç½²

## Feature Support Matrix

SGLang-JAX provides comprehensive TPU-optimized features for production LLM serving:

| Feature | Support Status | Description |
|---------|---------------|-------------|
| High-Throughput Continuous Batching | âœ… | Dynamic request batching for maximum TPU utilization |
| Radix Tree KV Cache | âœ… | Memory-efficient prefix sharing between requests |
| FlashAttention Backend | âœ… | TPU-optimized attention kernel for long sequences |
| Tensor Parallelism | âœ… | Distribute models across multiple TPU cores |
| Paged Attention | âœ… | Flexible KV cache management with paging |
| Speculative Decoding (EAGLE/EAGLE3) | âœ… | 20-40% throughput improvement for compatible models |
| Chunked Prefill | âœ… | Mixed prefill-decode batching |
| OpenAI-Compatible API | âœ… | Drop-in replacement for OpenAI API |
| Data Parallel Attention | ğŸš§ | In development - Attention computation with data parallelism |
| Quantization | ğŸš§ | In development - Model quantization for reduced memory usage |
| Multi-LoRA | ğŸš§ | In development - Serve multiple LoRA adapters simultaneously |

### Attention Backend Comparison

| Backend | Paged Attention | Spec Decoding | MLA | Sliding Window |
|---------|----------------|---------------|-----|----------------|
| FlashAttention (fa) | âœ… | âœ… | âŒ | âœ… |
| Native | âŒ | âŒ | âŒ | âŒ |

**NOTE:** FlashAttention backend is recommended for production workloads due to superior memory efficiency and performance.

**ä¸­æ–‡å¯¹ç…§**ï¼š## åŠŸèƒ½æ”¯æŒçŸ©é˜µ

SGLang-JAX ä¸ºç”Ÿäº§çº§ LLM æœåŠ¡æä¾›å…¨é¢çš„ TPU ä¼˜åŒ–åŠŸèƒ½ï¼š

| åŠŸèƒ½ | æ”¯æŒçŠ¶æ€ | æè¿° |
|---------|---------------|-------------|
| é«˜ååé‡è¿ç»­æ‰¹å¤„ç† | âœ… | åŠ¨æ€è¯·æ±‚æ‰¹å¤„ç†ä»¥æœ€å¤§åŒ– TPU åˆ©ç”¨ç‡ |
| Radix Tree KV ç¼“å­˜ | âœ… | è¯·æ±‚ä¹‹é—´å†…å­˜é«˜æ•ˆçš„å‰ç¼€å…±äº« |
| FlashAttention åç«¯ | âœ… | é’ˆå¯¹é•¿åºåˆ—ä¼˜åŒ–çš„ TPU æ³¨æ„åŠ›å†…æ ¸ |
| å¼ é‡å¹¶è¡Œ | âœ… | è·¨å¤šä¸ª TPU æ ¸å¿ƒåˆ†å¸ƒæ¨¡å‹ |
| åˆ†é¡µæ³¨æ„åŠ› | âœ… | ä½¿ç”¨åˆ†é¡µçš„çµæ´» KV ç¼“å­˜ç®¡ç† |
| æ¨æµ‹è§£ç  (EAGLE/EAGLE3) | âœ… | å…¼å®¹æ¨¡å‹ååé‡æå‡ 20-40% |
| å—çŠ¶é¢„å¡«å…… | âœ… | æ··åˆé¢„å¡«å……-è§£ç æ‰¹å¤„ç† |
| OpenAI å…¼å®¹ API | âœ… | OpenAI API çš„ç›´æ¥æ›¿ä»£å“ |
| æ•°æ®å¹¶è¡Œæ³¨æ„åŠ› | ğŸš§ | å¼€å‘ä¸­ - å¸¦æ•°æ®å¹¶è¡Œçš„æ³¨æ„åŠ›è®¡ç®— |
| é‡åŒ– | ğŸš§ | å¼€å‘ä¸­ - ç”¨äºå‡å°‘å†…å­˜ä½¿ç”¨çš„æ¨¡å‹é‡åŒ– |
| å¤š-LoRA | ğŸš§ | å¼€å‘ä¸­ - åŒæ—¶æœåŠ¡å¤šä¸ª LoRA é€‚é…å™¨ |

### æ³¨æ„åŠ›åç«¯æ¯”è¾ƒ

| åç«¯ | åˆ†é¡µæ³¨æ„åŠ› | æ¨æµ‹è§£ç  | MLA | æ»‘åŠ¨çª—å£ |
|---------|----------------|---------------|-----|----------------|
| FlashAttention (fa) | âœ… | âœ… | âŒ | âœ… |
| Native | âŒ | âŒ | âŒ | âŒ |

**æ³¨æ„ï¼š** ç”±äºå“è¶Šçš„å†…å­˜æ•ˆç‡å’Œæ€§èƒ½ï¼Œå»ºè®®ç”Ÿäº§å·¥ä½œè´Ÿè½½ä½¿ç”¨ FlashAttention åç«¯ã€‚

## Optimized Model List

The following models have been tested and optimized for TPU deployment:

| Model Family | Performance Status |
|--------------|-------------------|
| [Qwen 3](https://huggingface.co/Qwen) | â­ Recommended for production |
| [Qwen 3 MoE](https://huggingface.co/Qwen) | â­ Best performance |
| [Qwen 2](https://huggingface.co/Qwen) | Needs improvement |
| [Qwen 2 MoE](https://huggingface.co/Qwen) | Needs improvement |
| [Qwen 1.5](https://huggingface.co/Qwen) | Needs improvement |
| [Llama/LLaMA](https://huggingface.co/meta-llama) | Needs improvement |
| [Grok-2](https://huggingface.co/xai-org) | Needs improvement |
| [Gemma 2](https://huggingface.co/google) | Verified on TPU |
| Bailing MoE | Needs improvement |

**ä¸­æ–‡å¯¹ç…§**ï¼š## ä¼˜åŒ–æ¨¡å‹åˆ—è¡¨

ä»¥ä¸‹æ¨¡å‹å·²é’ˆå¯¹ TPU éƒ¨ç½²è¿›è¡Œäº†æµ‹è¯•å’Œä¼˜åŒ–ï¼š

| æ¨¡å‹ç³»åˆ— | æ€§èƒ½çŠ¶æ€ |
|--------------|-------------------|
| [Qwen 3](https://huggingface.co/Qwen) | â­ æ¨èç”¨äºç”Ÿäº§ |
| [Qwen 3 MoE](https://huggingface.co/Qwen) | â­ æœ€ä½³æ€§èƒ½ |
| [Qwen 2](https://huggingface.co/Qwen) | éœ€è¦æ”¹è¿› |
| [Qwen 2 MoE](https://huggingface.co/Qwen) | éœ€è¦æ”¹è¿› |
| [Qwen 1.5](https://huggingface.co/Qwen) | éœ€è¦æ”¹è¿› |
| [Llama/LLaMA](https://huggingface.co/meta-llama) | éœ€è¦æ”¹è¿› |
| [Grok-2](https://huggingface.co/xai-org) | éœ€è¦æ”¹è¿› |
| [Gemma 2](https://huggingface.co/google) | å·²åœ¨ TPU ä¸ŠéªŒè¯ |
| Bailing MoE | éœ€è¦æ”¹è¿› |

## Installation

### Method 1: Using PyPI (Recommended)

```bash
pip install sglang-jax
```

### Method 2: From Source

```bash
git clone https://github.com/sgl-project/sglang-jax
cd sglang-jax
uv venv --python 3.12 && source .venv/bin/activate
uv pip install -e "python[all]"
```

### Method 3: Using Docker

**NOTE:** Docker support for TPU is currently under development. Please use PyPI or source installation methods.

### Method 4: Cloud TPU with SkyPilot

[SkyPilot](https://github.com/skypilot-org/skypilot) provides simplified deployment on Google Cloud TPU:

1. Install SkyPilot and configure GCP access (see [SkyPilot documentation](https://skypilot.readthedocs.io/))

2. Create a SkyPilot configuration file:

<details>
<summary>SkyPilot YAML: <code>sglang-jax.sky.yaml</code></summary>

```yaml
# sglang-jax.sky.yaml
resources:
   accelerators: tpu-v6e-4
   accelerator_args:
      tpu_vm: True
      runtime_version: v2-alpha-tpuv6e

run: |
  git clone https://github.com/sgl-project/sglang-jax.git
  cd sglang-jax
  uv venv --python 3.12
  source .venv/bin/activate
  uv pip install -e "python[all]"
```

</details>

3. Launch your TPU cluster:

```bash
# Standard deployment
sky launch -c sglang-jax sglang-jax.sky.yaml --infra=gcp

# With spot instances for cost savings
sky launch -c sglang-jax sglang-jax.sky.yaml --infra=gcp --use-spot
```

**ä¸­æ–‡å¯¹ç…§**ï¼š## å®‰è£…

### æ–¹æ³• 1ï¼šä½¿ç”¨ PyPIï¼ˆæ¨èï¼‰

```bash
pip install sglang-jax
```

### æ–¹æ³• 2ï¼šä»æºç å®‰è£…

```bash
git clone https://github.com/sgl-project/sglang-jax
cd sglang-jax
uv venv --python 3.12 && source .venv/bin/activate
uv pip install -e "python[all]"
```

### æ–¹æ³• 3ï¼šä½¿ç”¨ Docker

**æ³¨æ„**ï¼šTPU çš„ Docker æ”¯æŒç›®å‰æ­£åœ¨å¼€å‘ä¸­ã€‚è¯·ä½¿ç”¨ PyPI æˆ–æºç å®‰è£…æ–¹æ³•ã€‚

### æ–¹æ³• 4ï¼šä½¿ç”¨ SkyPilot çš„äº‘ TPU

[SkyPilot](https://github.com/skypilot-org/skypilot) åœ¨ Google Cloud TPU ä¸Šæä¾›ç®€åŒ–çš„éƒ¨ç½²ï¼š

1. å®‰è£… SkyPilot å¹¶é…ç½® GCP è®¿é—®ï¼ˆè¯·å‚é˜… [SkyPilot æ–‡æ¡£](https://skypilot.readthedocs.io/)ï¼‰

2. åˆ›å»º SkyPilot é…ç½®æ–‡ä»¶ï¼š

<details>
<summary>SkyPilot YAML: <code>sglang-jax.sky.yaml</code></summary>

```yaml
# sglang-jax.sky.yaml
resources:
   accelerators: tpu-v6e-4
   accelerator_args:
      tpu_vm: True
      runtime_version: v2-alpha-tpuv6e

run: |
  git clone https://github.com/sgl-project/sglang-jax.git
  cd sglang-jax
  uv venv --python 3.12
  source .venv/bin/activate
  uv pip install -e "python[all]"
```

</details>

3. å¯åŠ¨æ‚¨çš„ TPU é›†ç¾¤ï¼š

```bash
# æ ‡å‡†éƒ¨ç½²
sky launch -c sglang-jax sglang-jax.sky.yaml --infra=gcp

# ä½¿ç”¨ç«ä»·å®ä¾‹ä»¥èŠ‚çœæˆæœ¬
sky launch -c sglang-jax sglang-jax.sky.yaml --infra=gcp --use-spot
```

## Launch of the Serving Engine

### Basic Example: Qwen-7B

```bash
JAX_COMPILATION_CACHE_DIR=/tmp/jit_cache python3 -u -m sgl_jax.launch_server \
    --model-path Qwen/Qwen-7B-Chat \
    --trust-remote-code \
    --dist-init-addr=0.0.0.0:10011 \
    --nnodes=1 \
    --tp-size=4 \
    --device=tpu \
    --random-seed=3 \
    --node-rank=0 \
    --mem-fraction-static=0.8 \
    --max-prefill-tokens=8192 \
    --download-dir=/tmp \
    --dtype=bfloat16 \
    --skip-server-warmup \
    --host 0.0.0.0 \
    --port 30000
```

**Key Parameters Explained:**

1. `JAX_COMPILATION_CACHE_DIR=/tmp/jit_cache` - Enables JIT compilation caching to accelerate server startup on subsequent runs
2. `--tp-size=4` - Tensor parallelism size; match this to your TPU core count (typically 1, 4, or 8)
3. `--device=tpu` - Specifies TPU device (this is the default for sglang-jax)
4. `--dtype=bfloat16` - Uses bfloat16 precision, which TPUs are optimized for
5. `--mem-fraction-static=0.8` - Allocates 80% of TPU HBM for static memory (adjustable from 0.2 to 0.9)
6. `--max-prefill-tokens=8192` - Maximum number of tokens processed in the prefill phase

**ä¸­æ–‡å¯¹ç…§**ï¼š## å¯åŠ¨æœåŠ¡å¼•æ“

### åŸºæœ¬ç¤ºä¾‹ï¼šQwen-7B

```bash
JAX_COMPILATION_CACHE_DIR=/tmp/jit_cache python3 -u -m sgl_jax.launch_server \
    --model-path Qwen/Qwen-7B-Chat \
    --trust-remote-code \
    --dist-init-addr=0.0.0.0:10011 \
    --nnodes=1 \
    --tp-size=4 \
    --device=tpu \
    --random-seed=3 \
    --node-rank=0 \
    --mem-fraction-static=0.8 \
    --max-prefill-tokens=8192 \
    --download-dir=/tmp \
    --dtype=bfloat16 \
    --skip-server-warmup \
    --host 0.0.0.0 \
    --port 30000
```

**å…³é”®å‚æ•°è¯´æ˜ï¼š**

1. `JAX_COMPILATION_CACHE_DIR=/tmp/jit_cache` - å¯ç”¨ JIT ç¼–è¯‘ç¼“å­˜ä»¥åŠ é€Ÿåç»­è¿è¡Œçš„æœåŠ¡å™¨å¯åŠ¨
2. `--tp-size=4` - å¼ é‡å¹¶è¡Œå¤§å°ï¼›å°†å…¶ä¸æ‚¨çš„ TPU æ ¸å¿ƒæ•°åŒ¹é…ï¼ˆé€šå¸¸ä¸º 1ã€4 æˆ– 8ï¼‰
3. `--device=tpu` - æŒ‡å®š TPU è®¾å¤‡ï¼ˆè¿™æ˜¯ sglang-jax çš„é»˜è®¤å€¼ï¼‰
4. `--dtype=bfloat16` - ä½¿ç”¨ bfloat16 ç²¾åº¦ï¼ŒTPU ä¸ºæ­¤è¿›è¡Œäº†ä¸“é—¨ä¼˜åŒ–
5. `--mem-fraction-static=0.8` - ä¸ºé™æ€å†…å­˜åˆ†é… 80% çš„ TPU HBMï¼ˆå¯ä» 0.2 è°ƒæ•´åˆ° 0.9ï¼‰
6. `--max-prefill-tokens=8192` - é¢„å¡«å……é˜¶æ®µå¤„ç†çš„æœ€å¤§ä»¤ç‰Œæ•°

### High-Performance Configuration: Qwen3-8B

For production workloads with optimal throughput:

```bash
python3 -u -m sgl_jax.launch_server \
    --model-path Qwen/Qwen3-8B \
    --trust-remote-code \
    --tp-size=4 \
    --device=tpu \
    --mem-fraction-static=0.8 \
    --chunked-prefill-size=2048 \
    --dtype=bfloat16 \
    --max-running-requests=256 \
    --page-size=128 \
    --attention-backend=fa
```

### Advanced: Speculative Decoding (EAGLE3)

Speculative decoding can improve throughput by 20-40% for compatible models:

```bash
python3 -u -m sgl_jax.launch_server \
    --model-path Qwen/Qwen3-32B \
    --trust-remote-code \
    --device=tpu \
    --tp-size=4 \
    --mem-fraction-static=0.8 \
    --max-prefill-tokens=4096 \
    --attention-backend=fa \
    --dtype=bfloat16 \
    --port=30000 \
    --host=0.0.0.0 \
    --disable-overlap-schedule \
    --speculative-algorithm=EAGLE3 \
    --speculative-draft-model-path=AngelSlim/Qwen3-32B_eagle3 \
    --page-size=64 \
    --speculative-eagle-topk=1 \
    --speculative-num-steps=3 \
    --speculative-num-draft-tokens=4
```

**NOTE:** Speculative decoding is currently supported for Qwen3 and LLaMA model families. See the [Speculative Decoding documentation](https://github.com/sgl-project/sglang-jax/blob/main/docs/features/speculative_decoding.md) for detailed configuration guidance.


### Multi-Node Distributed Serving

For large models requiring multiple TPU VMs:

```bash
# Node 0 (coordinator)
python3 -m sgl_jax.launch_server \
    --model-path MODEL_PATH \
    --dist-init-addr=NODE0_IP:10011 \
    --nnodes=2 \
    --node-rank=0 \
    --tp-size=8 \
    [other parameters...]

# Node 1 (worker)
python3 -m sgl_jax.launch_server \
    --model-path MODEL_PATH \
    --dist-init-addr=NODE0_IP:10011 \
    --nnodes=2 \
    --node-rank=1 \
    --tp-size=8 \
    [other parameters...]
```

## Benchmarking with Requests

### Throughput Testing

Basic throughput benchmark:

```bash
python3 -m sgl_jax.bench_serving \
    --backend sgl-jax \
    --dataset-name random \
    --num-prompts=100 \
    --random-input=512 \
    --random-output=128 \
    --max-concurrency=8 \
    --random-range-ratio=1 \
    --warmup-requests=0
```

### Latency Testing

Measure single-batch latency:

```bash
python3 -m sgl_jax.bench_one_batch_server \
    --base-url http://127.0.0.1:30000 \
    --model-path Qwen/Qwen-7B-Chat \
    --batch-size=32 \
    --input-len=256 \
    --output-len=32
```

### Comprehensive Benchmark Script

For systematic performance evaluation across different configurations:

```bash
#!/bin/bash
set -e

backend=${1:-sgl-jax}
num_prompts_per_concurrency=3
input_seq_lens=(1024 4096 8192)
output_seq_lens=(1 1024)
max_concurrencies=(8 16 32 64 128 256)

for input_seq_len in "${input_seq_lens[@]}"; do
    for output_seq_len in "${output_seq_lens[@]}"; do
        echo "======================================="
        echo "Testing ISL/OSL: $input_seq_len/$output_seq_len"
        echo "======================================="
        for max_concurrency in "${max_concurrencies[@]}"; do
            num_prompts=$((num_prompts_per_concurrency * max_concurrency))
            python3 -m sgl_jax.bench_serving \
                --backend ${backend} \
                --dataset-name random \
                --num-prompts ${num_prompts} \
                --random-input ${input_seq_len} \
                --random-output ${output_seq_len} \
                --max-concurrency ${max_concurrency} \
                --random-range-ratio 1 \
                --disable-ignore-eos \
                --warmup-requests 0
        done
    done
done
```

For detailed help on all benchmark parameters:

```bash
python3 -m sgl_jax.bench_serving --help
```

See the [Benchmark and Profiling Guide](https://github.com/sgl-project/sglang-jax/blob/main/docs/developer_guide/benchmark_and_profiling.md) for advanced benchmarking techniques and profiling with JAX Profiler.

**ä¸­æ–‡å¯¹ç…§**ï¼š## ä½¿ç”¨è¯·æ±‚è¿›è¡ŒåŸºå‡†æµ‹è¯•

### ååé‡æµ‹è¯•

åŸºæœ¬ååé‡åŸºå‡†æµ‹è¯•ï¼š

```bash
python3 -m sgl_jax.bench_serving \
    --backend sgl-jax \
    --dataset-name random \
    --num-prompts=100 \
    --random-input=512 \
    --random-output=128 \
    --max-concurrency=8 \
    --random-range-ratio=1 \
    --warmup-requests=0
```

### å»¶è¿Ÿæµ‹è¯•

æµ‹é‡å•æ‰¹æ¬¡å»¶è¿Ÿï¼š

```bash
python3 -m sgl_jax.bench_one_batch_server \
    --base-url http://127.0.0.1:30000 \
    --model-path Qwen/Qwen-7B-Chat \
    --batch-size=32 \
    --input-len=256 \
    --output-len=32
```

### ç»¼åˆåŸºå‡†æµ‹è¯•è„šæœ¬

ç”¨äºå¯¹ä¸åŒé…ç½®è¿›è¡Œç³»ç»Ÿçš„æ€§èƒ½è¯„ä¼°ï¼š

```bash
#!/bin/bash
set -e

backend=${1:-sgl-jax}
num_prompts_per_concurrency=3
input_seq_lens=(1024 4096 8192)
output_seq_lens=(1 1024)
max_concurrencies=(8 16 32 64 128 256)

for input_seq_len in "${input_seq_lens[@]}"; do
    for output_seq_len in "${output_seq_lens[@]}"; do
        echo "======================================="
        echo "Testing ISL/OSL: $input_seq_len/$output_seq_len"
        echo "======================================="
        for max_concurrency in "${max_concurrencies[@]}"; do
            num_prompts=$((num_prompts_per_concurrency * max_concurrency))
            python3 -m sgl_jax.bench_serving \
                --backend ${backend} \
                --dataset-name random \
                --num-prompts ${num_prompts} \
                --random-input ${input_seq_len} \
                --random-output ${output_seq_len} \
                --max-concurrency ${max_concurrency} \
                --random-range-ratio 1 \
                --disable-ignore-eos \
                --warmup-requests 0
        done
    done
done
```

æœ‰å…³æ‰€æœ‰åŸºå‡†æµ‹è¯•å‚æ•°çš„è¯¦ç»†å¸®åŠ©ï¼š

```bash
python3 -m sgl_jax.bench_serving --help
```

è¯·å‚é˜…[åŸºå‡†æµ‹è¯•å’Œåˆ†ææŒ‡å—](https://github.com/sgl-project/sglang-jax/blob/main/docs/developer_guide/benchmark_and_profiling.md)äº†è§£é«˜çº§åŸºå‡†æµ‹è¯•æŠ€æœ¯å’Œ JAX Profiler åˆ†æã€‚

## Performance Optimization

### Memory Optimization

**Reduce memory usage:**
- Lower `--mem-fraction-static` (from 0.8 â†’ 0.5 â†’ 0.3)
- Decrease `--max-prefill-tokens` (from 16384 â†’ 8192 â†’ 4096)
- Reduce `--max-running-requests`

**Handle OOM errors:**
- Start with conservative memory settings (`--mem-fraction-static=0.5`)
- Gradually increase until you find the optimal balance
- Increase `--page-size` for better memory locality (1 â†’ 16 â†’ 64 â†’ 128)

### Throughput Optimization

To maximize tokens per second:

- Use FlashAttention backend: `--attention-backend=fa`
- Enable speculative decoding (EAGLE3) for Qwen3 models (20-40% improvement)
- Increase `--max-running-requests` to 256+
- Set `--mem-fraction-static` to 0.8+ (if memory allows)
- Use larger page sizes (64-128)
- Enable chunked prefill: `--chunked-prefill-size=2048`

### Latency Optimization

To minimize time-to-first-token (TTFT) and inter-token latency:

- Reduce `--page-size` to 1-4
- Lower `--max-running-requests` (16-32) for smaller batches
- Reduce `--chunked-prefill-size`
- Use conservative memory settings to avoid GC pauses

### TPU-Specific Optimizations

1. **JIT Compilation Cache:**
   ```bash
   export JAX_COMPILATION_CACHE_DIR=/tmp/jit_cache
   ```
   Always set this environment variable to cache compiled kernels and accelerate server startup.

2. **Data Type Optimization:**
   Use `--dtype=bfloat16` for TPU native optimization. TPUs are specifically designed for bfloat16 computations.

3. **Tensor Parallelism:**
   Match `--tp-size` to your TPU core configuration (1, 4, or 8) for optimal model distribution.

4. **Attention Backend:**
   Always use `--attention-backend=fa` (FlashAttention) for production workloads.

## Troubleshooting

### OOM (Out of Memory) Errors

If you encounter out-of-memory errors:

1. Reduce `--mem-fraction-static` from 0.8 to 0.5 or lower
2. Decrease `--max-prefill-tokens` from 8192 to 4096 or 2048
3. Lower `--max-running-requests` to reduce concurrent batch size
4. Increase `--page-size` for better memory layout efficiency

### Compilation Long-Time

If the server takes too long to start:

1. Ensure `JAX_COMPILATION_CACHE_DIR` is properly set
2. Understand that the first run requires JIT compilation (this is normal)
3. Subsequent runs will be significantly faster with cached compilations
4. Consider using `--skip-server-warmup` to defer compilation until first request

### Low Throughput

If you're not achieving expected throughput:

1. Verify `--tp-size` matches your TPU core configuration
2. Check that `--attention-backend=fa` is enabled
3. Increase `--max-running-requests` to enable larger batch formation
4. Consider enabling speculative decoding for compatible models
5. Ensure memory settings allow for sufficient batch sizes

### Connection Issues

If clients cannot connect to the server:

1. Ensure `--host=0.0.0.0` for external access (not just `127.0.0.1`)
2. Verify firewall rules allow traffic on the specified port (default: 30000)
3. Check that the server process is running: `curl http://localhost:30000/health`

## Advanced Features

### Speculative Decoding

SGLang-JAX supports EAGLE and EAGLE3 speculative decoding algorithms for Qwen3 and LLaMA model families. Speculative decoding can improve throughput by 20-40% without affecting output quality.

See the [Speculative Decoding documentation](https://github.com/sgl-project/sglang-jax/blob/main/docs/features/speculative_decoding.md) for detailed configuration and supported model combinations.

### Chunked Prefill

Enable mixed prefill-decode batching for better TPU utilization:

```bash
--chunked-prefill-size=2048 --enable-mixed-chunk
```

This allows the scheduler to mix prefill operations with decode operations in the same batch, improving overall throughput.

### Custom Attention Backends

SGLang-JAX supports a plugin-based attention backend system. You can implement custom attention kernels optimized for specific use cases.

See the [Attention Backend documentation](https://github.com/sgl-project/sglang-jax/blob/main/docs/features/attention_backend.md) for implementation details.

### Environment Verification

Verify your TPU setup before deploying:

```bash
python -c "from sgl_jax import check_env; check_env.check_env()"
```

This command checks:
- Installed package versions
- TPU device availability and specifications
- System resources and configuration
- Compatibility of settings

## Contributing

We welcome contributions to improve TPU support in SGLang-JAX!

### Areas for Contribution

**Check the [Development Roadmap](https://github.com/sgl-project/sglang-jax/issues/190)** to see planned features and find opportunities to contribute new functionality.

Current contribution areas include:

- Performance optimizations for specific TPU generations
- Support for additional model architectures
- Documentation improvements and examples
- Bug reports and fixes
- Benchmark results and performance analysis

### How to Contribute

1. Visit the [sglang-jax repository](https://github.com/sgl-project/sglang-jax)
2. Read the [Contribution Guide](https://github.com/sgl-project/sglang-jax/blob/main/docs/developer_guide/contribution_guide.md)
3. Join the [SGL-JAX Slack community](https://sgl-fru7574.slack.com/archives/C09EBE5HT5X) for discussions
4. Report issues at [sglang-jax/issues](https://github.com/sgl-project/sglang-jax/issues)

### Testing on TPU

For contributors who need TPU access for testing:

- Refer to the [TPU Resources Guide](https://github.com/sgl-project/sglang-jax/blob/main/docs/developer_guide/tpu_resources_guide.md) for information on accessing TPU hardware
- Use SkyPilot with spot instances for cost-effective testing
- Follow the [Benchmark and Profiling Guide](https://github.com/sgl-project/sglang-jax/blob/main/docs/developer_guide/benchmark_and_profiling.md) for performance validation

## References

### Documentation

- [SGLang-JAX Repository](https://github.com/sgl-project/sglang-jax)
- [SGLang-JAX Installation Guide](https://github.com/sgl-project/sglang-jax/blob/main/docs/get_started/install.md)
- [Qwen Models Quick Start](https://github.com/sgl-project/sglang-jax/blob/main/docs/basic_usage/qwen.md)
- [Benchmark and Profiling Guide](https://github.com/sgl-project/sglang-jax/blob/main/docs/developer_guide/benchmark_and_profiling.md)
- [Speculative Decoding](https://github.com/sgl-project/sglang-jax/blob/main/docs/features/speculative_decoding.md)

### External Resources

- [JAX Documentation](https://jax.readthedocs.io/)
- [Google Cloud TPU Documentation](https://cloud.google.com/tpu/docs)
- [SkyPilot Documentation](https://skypilot.readthedocs.io/)

## ä»£ç å®ç°

### æ ¸å¿ƒæ–‡ä»¶

SGLang TPU æ”¯æŒé€šè¿‡ç‹¬ç«‹çš„ [sglang-jax](https://github.com/sgl-project/sglang-jax) ä»“åº“å®ç°ã€‚

| æ–‡ä»¶ | ä½œç”¨ |
|------|------|
| `sgl_jax/launch_server.py` | TPU æœåŠ¡å™¨å¯åŠ¨å™¨ï¼šåŸºäº JAX çš„æ¨ç†å¼•æ“å…¥å£ç‚¹ |
| `sgl_jax/bench_serving.py` | TPU åŸºå‡†æµ‹è¯•è„šæœ¬ï¼šååé‡å’Œå»¶è¿Ÿæµ‹é‡ |
| `sgl_jax/bench_one_batch_server.py` | TPU çš„å•æ‰¹æ¬¡å»¶è¿ŸåŸºå‡†æµ‹è¯• |

### å…³é”®ä»£ç é€»è¾‘

- **JAX åç«¯**ï¼šå®Œå…¨ç‹¬ç«‹äºä¸» CUDA/PyTorch ä»£ç åº“ï¼›ä½¿ç”¨ JAX è¿›è¡Œ TPU åŸç”Ÿè®¡ç®—
- **JIT ç¼–è¯‘ç¼“å­˜**ï¼š`JAX_COMPILATION_CACHE_DIR` ç¼“å­˜ç¼–è¯‘çš„ TPU å†…æ ¸ï¼Œä»¥åŠ å¿«åç»­å¯åŠ¨
- **TPU ä¸Šçš„ FlashAttention**ï¼šé€šè¿‡ `--attention-backend=fa` ä½¿ç”¨è‡ªå®šä¹‰ TPU ä¼˜åŒ–æ³¨æ„åŠ›å†…æ ¸
- **æ¨æµ‹è§£ç **ï¼šæ”¯æŒ Qwen3 å’Œ LLaMA ç³»åˆ—çš„ EAGLE/EAGLE3ï¼ˆååé‡æå‡ 20-40%ï¼‰

### é›†æˆè¦ç‚¹

- **å®‰è£…**ï¼š`pip install sglang-jax`ï¼ˆç‹¬ç«‹ PyPI åŒ…ï¼‰æˆ–ä» `sgl-project/sglang-jax` æºç æ„å»º
- **æœåŠ¡å™¨å¯åŠ¨**ï¼š`python3 -m sgl_jax.launch_server`ï¼ˆæ³¨æ„ï¼š`sgl_jax` æ¨¡å—ï¼Œè€Œé `sglang`ï¼‰
- **åŸºå‡†æµ‹è¯•**ï¼š`python3 -m sgl_jax.bench_serving --backend sgl-jax`ï¼ˆä¸“ç”¨åç«¯åç§°ï¼‰
- **TPU å¹¶è¡Œ**ï¼š`--tp-size` åŒ¹é… TPU æ ¸å¿ƒæ•°ï¼ˆ1ã€4 æˆ– 8ï¼‰ï¼›é€šè¿‡ `--nnodes` å’Œ `--node-rank` å¤šèŠ‚ç‚¹
